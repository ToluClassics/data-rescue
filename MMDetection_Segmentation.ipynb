{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "MMDetection Segmentation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RBYoyJ3bXfR",
        "outputId": "7af8b44b-0d88-49df-f379-4097d151b377"
      },
      "source": [
        "import os\n",
        "\n",
        "#split data into train test \n",
        "\n",
        "image_path = '/content/drive/MyDrive/data/VOC/jpegimages'\n",
        "\n",
        "all_file_list = [file.replace('.png','') for file in os.listdir(image_path)]\n",
        "\n",
        "print(len(all_file_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "134\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZPHGIrgeR4M"
      },
      "source": [
        "import random\n",
        "\n",
        "all_file_list = random.sample(all_file_list, len(all_file_list))\n",
        "\n",
        "base_path = '/content/drive/MyDrive/data'\n",
        "\n",
        "train_images = 'train.txt'\n",
        "test_images = 'test.txt'\n",
        "val_images = 'val.txt'\n",
        "\n",
        "with open(os.path.join(base_path,train_images),'w') as f:\n",
        "    for item in all_file_list[:100]:\n",
        "        f.writelines(item+'\\n')\n",
        "\n",
        "\n",
        "with open(os.path.join(base_path,test_images),'w') as f:\n",
        "    for item in all_file_list[100:120]:\n",
        "        f.writelines(item+'\\n')\n",
        "\n",
        "\n",
        "with open(os.path.join(base_path,val_images),'w') as f:\n",
        "    for item in all_file_list[120:]:\n",
        "        f.writelines(item+'\\n')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8F1tfIhv6DW",
        "outputId": "da7e852f-22b9-4be5-fb72-44c5651f1b59"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "base_path = '/content/drive/MyDrive/data'\n",
        "anno_path = os.path.join(base_path, \"VOC/annotation\")\n",
        "classes_names = []\n",
        "xml_list = []\n",
        "print(anno_path)\n",
        "for xml_file in glob.glob(anno_path + \"/*.xml\"):\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "    for member in root.findall(\"object\"):\n",
        "        classes_names.append(member[0].text)\n",
        "classes_names = list(set(classes_names))\n",
        "classes_names.sort()\n",
        "classes_names"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/data/VOC/annotation\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['station_names', 'table']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIMjBfi4xjqJ"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import json\n",
        "import xml.etree.ElementTree as ET\n",
        "import glob\n",
        "\n",
        "START_BOUNDING_BOX_ID = 1\n",
        "PRE_DEFINE_CATEGORIES = None\n",
        "# If necessary, pre-define category and its id\n",
        "#  PRE_DEFINE_CATEGORIES = {\"aeroplane\": 1, \"bicycle\": 2, \"bird\": 3, \"boat\": 4,\n",
        "#  \"bottle\":5, \"bus\": 6, \"car\": 7, \"cat\": 8, \"chair\": 9,\n",
        "#  \"cow\": 10, \"diningtable\": 11, \"dog\": 12, \"horse\": 13,\n",
        "#  \"motorbike\": 14, \"person\": 15, \"pottedplant\": 16,\n",
        "#  \"sheep\": 17, \"sofa\": 18, \"train\": 19, \"tvmonitor\": 20}\n",
        "\n",
        "\n",
        "def get(root, name):\n",
        "    vars = root.findall(name)\n",
        "    return vars\n",
        "\n",
        "\n",
        "def get_and_check(root, name, length):\n",
        "    vars = root.findall(name)\n",
        "    if len(vars) == 0:\n",
        "        raise ValueError(\"Can not find %s in %s.\" % (name, root.tag))\n",
        "    if length > 0 and len(vars) != length:\n",
        "        raise ValueError(\n",
        "            \"The size of %s is supposed to be %d, but is %d.\"\n",
        "            % (name, length, len(vars))\n",
        "        )\n",
        "    if length == 1:\n",
        "        vars = vars[0]\n",
        "    return vars\n",
        "\n",
        "\n",
        "def get_filename_as_int(filename):\n",
        "    try:\n",
        "        filename = filename.replace(\"\\\\\", \"/\").replace('page','')\n",
        "        filename = os.path.splitext(os.path.basename(filename))[0]\n",
        "        return int(filename)\n",
        "    except:\n",
        "        raise ValueError(\"Filename %s is supposed to be an integer.\" % (filename))\n",
        "\n",
        "\n",
        "def get_categories(xml_files):\n",
        "    \"\"\"Generate category name to id mapping from a list of xml files.\n",
        "    \n",
        "    Arguments:\n",
        "        xml_files {list} -- A list of xml file paths.\n",
        "    \n",
        "    Returns:\n",
        "        dict -- category name to id mapping.\n",
        "    \"\"\"\n",
        "    classes_names = []\n",
        "    for xml_file in xml_files:\n",
        "        tree = ET.parse(xml_file)\n",
        "        root = tree.getroot()\n",
        "        for member in root.findall(\"object\"):\n",
        "            classes_names.append(member[0].text)\n",
        "    classes_names = list(set(classes_names))\n",
        "    classes_names.sort()\n",
        "    return {name: i for i, name in enumerate(classes_names)}\n",
        "\n",
        "\n",
        "def convert(xml_files, json_file):\n",
        "    json_dict = {\"images\": [], \"type\": \"instances\", \"annotations\": [], \"categories\": []}\n",
        "    if PRE_DEFINE_CATEGORIES is not None:\n",
        "        categories = PRE_DEFINE_CATEGORIES\n",
        "    else:\n",
        "        categories = get_categories(xml_files)\n",
        "    bnd_id = START_BOUNDING_BOX_ID\n",
        "    for xml_file in xml_files:\n",
        "        tree = ET.parse(xml_file)\n",
        "        root = tree.getroot()\n",
        "        path = get(root, \"path\")\n",
        "        if len(path) == 1:\n",
        "            filename = os.path.basename(path[0].text)\n",
        "        elif len(path) == 0:\n",
        "            filename = get_and_check(root, \"filename\", 1).text\n",
        "        else:\n",
        "            raise ValueError(\"%d paths found in %s\" % (len(path), xml_file))\n",
        "        ## The filename must be a number\n",
        "        image_id = get_filename_as_int(filename)\n",
        "        size = get_and_check(root, \"size\", 1)\n",
        "        width = int(get_and_check(size, \"width\", 1).text)\n",
        "        height = int(get_and_check(size, \"height\", 1).text)\n",
        "        image = {\n",
        "            \"file_name\": os.path.basename(filename.replace(\"\\\\\",\"/\")),\n",
        "            \"height\": height,\n",
        "            \"width\": width,\n",
        "            \"id\": image_id,\n",
        "        }\n",
        "        json_dict[\"images\"].append(image)\n",
        "        ## Currently we do not support segmentation.\n",
        "        #  segmented = get_and_check(root, 'segmented', 1).text\n",
        "        #  assert segmented == '0'\n",
        "        for obj in get(root, \"object\"):\n",
        "            category = get_and_check(obj, \"name\", 1).text\n",
        "            if category not in categories:\n",
        "                new_id = len(categories)\n",
        "                categories[category] = new_id\n",
        "            category_id = categories[category]\n",
        "            bndbox = get_and_check(obj, \"bndbox\", 1)\n",
        "            xmin = int(get_and_check(bndbox, \"xmin\", 1).text) - 1\n",
        "            ymin = int(get_and_check(bndbox, \"ymin\", 1).text) - 1\n",
        "            xmax = int(get_and_check(bndbox, \"xmax\", 1).text)\n",
        "            ymax = int(get_and_check(bndbox, \"ymax\", 1).text)\n",
        "            assert xmax > xmin\n",
        "            assert ymax > ymin\n",
        "            o_width = abs(xmax - xmin)\n",
        "            o_height = abs(ymax - ymin)\n",
        "            ann = {\n",
        "                \"area\": o_width * o_height,\n",
        "                \"iscrowd\": 0,\n",
        "                \"image_id\": image_id,\n",
        "                \"bbox\": [xmin, ymin, o_width, o_height],\n",
        "                \"category_id\": category_id,\n",
        "                \"id\": bnd_id,\n",
        "                \"ignore\": 0,\n",
        "                \"segmentation\": [],\n",
        "            }\n",
        "            json_dict[\"annotations\"].append(ann)\n",
        "            bnd_id = bnd_id + 1\n",
        "\n",
        "    for cate, cid in categories.items():\n",
        "        cat = {\"supercategory\": \"none\", \"id\": cid, \"name\": cate}\n",
        "        json_dict[\"categories\"].append(cat)\n",
        "\n",
        "    os.makedirs(os.path.dirname(json_file), exist_ok=True)\n",
        "    json_fp = open(json_file, \"w\")\n",
        "    json_str = json.dumps(json_dict)\n",
        "    json_fp.write(json_str)\n",
        "    json_fp.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xakFmR-UyXuH"
      },
      "source": [
        "xml_dir = base_path+\"/VOC/annotation\"\n",
        "xml_files = glob.glob(os.path.join(xml_dir, \"*.xml\"))\n",
        "\n",
        "train_split_file = \"/content/drive/MyDrive/data/train.txt\"\n",
        "test_split_file = \"/content/drive/MyDrive/data/test.txt\"\n",
        "val_split_file = \"/content/drive/MyDrive/data/val.txt\"\n",
        "\n",
        "def get_split_xml_files(xml_files, split_file):\n",
        "    with open(split_file) as f:\n",
        "        file_names = [line.strip() for line in f.readlines()]\n",
        "    return [xml_file for xml_file in xml_files if os.path.splitext(os.path.basename(xml_file))[0] in file_names]\n",
        "\n",
        "train_xml_files = get_split_xml_files(xml_files, train_split_file)\n",
        "test_xml_files = get_split_xml_files(xml_files, test_split_file)\n",
        "val_xml_files = get_split_xml_files(xml_files, val_split_file)\n",
        "\n",
        "convert(train_xml_files, base_path + \"/coco/train.json\")\n",
        "convert(test_xml_files, base_path + \"/coco/test.json\")\n",
        "convert(val_xml_files, base_path + \"/coco/val.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "O5WU1-UM0GFN",
        "outputId": "82a92c0d-ae49-4586-d79f-0042867f2dda"
      },
      "source": [
        "# install dependencies: (use cu101 because colab has CUDA 10.1)\n",
        "!pip install -U torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# install mmcv-full thus we could use CUDA operators\n",
        "!pip install mmcv-full\n",
        "\n",
        "# install Pillow 7.0.0 back in order to avoid bug in colab\n",
        "!pip install Pillow==7.0.0\n",
        "\n",
        "!pip install mmdet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.5.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.5.1%2Bcu101-cp37-cp37m-linux_x86_64.whl (704.4MB)\n",
            "\u001b[K     |████████████████████████████████| 704.4MB 27kB/s \n",
            "\u001b[?25hCollecting torchvision==0.6.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.6.1%2Bcu101-cp37-cp37m-linux_x86_64.whl (6.6MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 32.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.7/dist-packages (from torch==1.5.1+cu101) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.5.1+cu101) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.6.1+cu101) (7.1.2)\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.5.1+cu101 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.8.1+cu101\n",
            "    Uninstalling torch-1.8.1+cu101:\n",
            "      Successfully uninstalled torch-1.8.1+cu101\n",
            "  Found existing installation: torchvision 0.9.1+cu101\n",
            "    Uninstalling torchvision-0.9.1+cu101:\n",
            "      Successfully uninstalled torchvision-0.9.1+cu101\n",
            "Successfully installed torch-1.5.1+cu101 torchvision-0.6.1+cu101\n",
            "Collecting mmcv-full\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b5/3c/344626bf6e39b98250e2bfb54bcbd40514be7c9c3f37d972db7c7b050018/mmcv-full-1.3.5.tar.gz (303kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 18.8MB/s \n",
            "\u001b[?25hCollecting addict\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/00/b08f23b7d7e1e14ce01419a467b583edbb93c6cdb8654e54a9cc579cd61f/addict-2.4.0-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mmcv-full) (1.19.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from mmcv-full) (7.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from mmcv-full) (3.13)\n",
            "Collecting yapf\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5f/0d/8814e79eb865eab42d95023b58b650d01dec6f8ea87fc9260978b1bf2167/yapf-0.31.0-py2.py3-none-any.whl (185kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 41.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: mmcv-full\n",
            "  Building wheel for mmcv-full (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mmcv-full: filename=mmcv_full-1.3.5-cp37-cp37m-linux_x86_64.whl size=25617465 sha256=4e1580d05a9e8f0dc0dc1cc3a7cbee92651b5fe00628ff87f575ecdf43c7748c\n",
            "  Stored in directory: /root/.cache/pip/wheels/06/1b/49/7a6ca2a423aa7ad9cbd2caf83f10e0ee09aa06e109994dcc0b\n",
            "Successfully built mmcv-full\n",
            "Installing collected packages: addict, yapf, mmcv-full\n",
            "Successfully installed addict-2.4.0 mmcv-full-1.3.5 yapf-0.31.0\n",
            "Collecting Pillow==7.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/79/b2d5695d1a931474fa68b68ec93bdf08ba9acbc4d6b3b628eb6aac81d11c/Pillow-7.0.0-cp37-cp37m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 26.8MB/s \n",
            "\u001b[31mERROR: bokeh 2.3.2 has requirement pillow>=7.1.0, but you'll have pillow 7.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Pillow\n",
            "  Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "Successfully installed Pillow-7.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting mmdet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/c9/107507a9a66394f8d031c11b01de1384be24fc629c989fec15b765cbf3a4/mmdet-2.12.0-py3-none-any.whl (592kB)\n",
            "\r\u001b[K     |▌                               | 10kB 21.0MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 27.1MB/s eta 0:00:01\r\u001b[K     |█▋                              | 30kB 32.2MB/s eta 0:00:01\r\u001b[K     |██▏                             | 40kB 34.2MB/s eta 0:00:01\r\u001b[K     |██▊                             | 51kB 25.8MB/s eta 0:00:01\r\u001b[K     |███▎                            | 61kB 28.1MB/s eta 0:00:01\r\u001b[K     |███▉                            | 71kB 26.1MB/s eta 0:00:01\r\u001b[K     |████▍                           | 81kB 27.2MB/s eta 0:00:01\r\u001b[K     |█████                           | 92kB 26.1MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 102kB 27.5MB/s eta 0:00:01\r\u001b[K     |██████                          | 112kB 27.5MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 122kB 27.5MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 133kB 27.5MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 143kB 27.5MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 153kB 27.5MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 163kB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 174kB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████                      | 184kB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 194kB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 204kB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 215kB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 225kB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 235kB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 245kB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 256kB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 266kB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 276kB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 286kB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████████                | 296kB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 307kB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 317kB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 327kB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 337kB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 348kB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 358kB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 368kB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 378kB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 389kB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 399kB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 409kB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 419kB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 430kB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 440kB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 450kB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 460kB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 471kB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 481kB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 491kB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 501kB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 512kB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 522kB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 532kB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 542kB 27.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 552kB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 563kB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 573kB 27.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 583kB 27.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 593kB 27.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mmdet) (1.19.5)\n",
            "Collecting terminaltables\n",
            "  Downloading https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from mmdet) (1.15.0)\n",
            "Requirement already satisfied: pycocotools; platform_system == \"Linux\" in /usr/local/lib/python3.7/dist-packages (from mmdet) (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mmdet) (3.2.2)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.7/dist-packages (from pycocotools; platform_system == \"Linux\"->mmdet) (0.29.23)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools; platform_system == \"Linux\"->mmdet) (56.1.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mmdet) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mmdet) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mmdet) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mmdet) (2.4.7)\n",
            "Building wheels for collected packages: terminaltables\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for terminaltables: filename=terminaltables-3.1.0-cp37-none-any.whl size=15356 sha256=bc6a3988d409a170dff66accc2e18ee1b2c8bb718eaf34f93cab4eb81df1011b\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/6b/50/6c75775b681fb36cdfac7f19799888ef9d8813aff9e379663e\n",
            "Successfully built terminaltables\n",
            "Installing collected packages: terminaltables, mmdet\n",
            "Successfully installed mmdet-2.12.0 terminaltables-3.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I44HeJv86Ow1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4eb2c46a-23c5-4fe3-c837-94be75e21bd4"
      },
      "source": [
        "# Install mmdetection\n",
        "!rm -rf mmdetection\n",
        "!git clone https://github.com/open-mmlab/mmdetection.git\n",
        "%cd mmdetection\n",
        "\n",
        "!pip install -e ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'mmdetection'...\n",
            "remote: Enumerating objects: 18028, done.\u001b[K\n",
            "remote: Counting objects: 100% (124/124), done.\u001b[K\n",
            "remote: Compressing objects: 100% (101/101), done.\u001b[K\n",
            "remote: Total 18028 (delta 45), reused 67 (delta 23), pack-reused 17904\u001b[K\n",
            "Receiving objects: 100% (18028/18028), 21.40 MiB | 33.87 MiB/s, done.\n",
            "Resolving deltas: 100% (12521/12521), done.\n",
            "/content/mmdetection\n",
            "Obtaining file:///content/mmdetection\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mmdet==2.12.0) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mmdet==2.12.0) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from mmdet==2.12.0) (1.15.0)\n",
            "Requirement already satisfied: terminaltables in /usr/local/lib/python3.7/dist-packages (from mmdet==2.12.0) (3.1.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.7/dist-packages (from mmdet==2.12.0) (2.0.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mmdet==2.12.0) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mmdet==2.12.0) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mmdet==2.12.0) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mmdet==2.12.0) (2.4.7)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools->mmdet==2.12.0) (56.1.0)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.7/dist-packages (from pycocotools->mmdet==2.12.0) (0.29.23)\n",
            "Installing collected packages: mmdet\n",
            "  Found existing installation: mmdet 2.12.0\n",
            "    Uninstalling mmdet-2.12.0:\n",
            "      Successfully uninstalled mmdet-2.12.0\n",
            "  Running setup.py develop for mmdet\n",
            "Successfully installed mmdet\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BtE3kTQ17CJ",
        "outputId": "27525227-f8d1-4a62-d1c6-c8c84a4e16b6"
      },
      "source": [
        "# Check Pytorch installation\n",
        "import torch, torchvision\n",
        "print(torch.__version__, torch.cuda.is_available())\n",
        "\n",
        "# Check MMDetection installation±\n",
        "import mmdet\n",
        "print(mmdet.__version__)\n",
        "\n",
        "# Check mmcv installation\n",
        "from mmcv.ops import get_compiling_cuda_version, get_compiler_version\n",
        "print(get_compiling_cuda_version())\n",
        "print(get_compiler_version())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.5.1+cu101 True\n",
            "2.12.0\n",
            "11.0\n",
            "GCC 7.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0q3J4VYS-wdX"
      },
      "source": [
        "from mmcv import Config\n",
        "from mmdet.apis import set_random_seed\n",
        "from mmdet.datasets import build_dataset\n",
        "from mmdet.models import build_detector\n",
        "from mmdet.apis import train_detector, init_detector, inference_detector, show_result_pyplot\n",
        "import torch "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iPg6fH7KiZi",
        "outputId": "f2dc151a-42bf-46cb-a542-989603ed4bdb"
      },
      "source": [
        "!gdown \"https://drive.google.com/u/0/uc?id=1-QieHkR1Q7CXuBu4fp3rYrvDG9j26eFT\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/u/0/uc?id=1-QieHkR1Q7CXuBu4fp3rYrvDG9j26eFT\n",
            "To: /content/epoch_36.pth\n",
            "664MB [00:07, 90.3MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty9p2z21CQKM"
      },
      "source": [
        "cfg = Config.fromfile('/content/drive/MyDrive/data/Training_config.py')\n",
        "cfg.dataset_type = 'CocoDataset'\n",
        "cfg.classes = (\"station_names\", \"table\",)\n",
        "# number of classes\n",
        "\n",
        "cfg.model.roi_head.bbox_head[0].num_classes = 2\n",
        "cfg.model.roi_head.bbox_head[1].num_classes = 2\n",
        "cfg.model.roi_head.bbox_head[2].num_classes = 2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-7YvNBcPaSD"
      },
      "source": [
        "PREFIX = 'VOC/jpegimages'\n",
        "\n",
        "\n",
        "cfg.data.train.img_prefix = PREFIX\n",
        "cfg.data.train.data_root = '/content/drive/MyDrive/data'\n",
        "cfg.data.train.ann_file = 'coco/train.json'\n",
        "cfg.data.train.classes = cfg.classes\n",
        "cfg.data.train.type = 'CocoDataset'\n",
        "\n",
        "cfg.data.val.img_prefix = PREFIX\n",
        "cfg.data.val.data_root = '/content/drive/MyDrive/data'\n",
        "cfg.data.val.ann_file = 'coco/val.json'\n",
        "cfg.data.val.classes = cfg.classes\n",
        "cfg.data.val.type = 'CocoDataset'\n",
        "\n",
        "cfg.data.test.img_prefix = PREFIX\n",
        "cfg.data.test.data_root = '/content/drive/MyDrive/data'\n",
        "cfg.data.test.ann_file = 'coco/test.json'\n",
        "cfg.data.test.classes = cfg.classes\n",
        "cfg.data.test.type = 'CocoDataset'\n",
        "\n",
        "\n",
        "cfg.load_from = '/content/drive/MyDrive/data/init_model/epoch_36.pth'\n",
        "cfg.work_dir = \"/content/drive/MyDrive/data/work_dir\"\n",
        "\n",
        "cfg.optimizer.lr = 0.02 / 8\n",
        "cfg.lr_config.warmup = None\n",
        "cfg.log_config.interval = 600\n",
        "\n",
        "\n",
        "# We can set the evaluation interval to reduce the evaluation times\n",
        "cfg.evaluation.interval = 3\n",
        "cfg.evaluation.classwise = True\n",
        "# We can set the checkpoint saving interval to reduce the storage cost\n",
        "cfg.checkpoint_config.interval = 3\n",
        "# Set seed thus the results are more reproducible\n",
        "cfg.seed = 0\n",
        "set_random_seed(0, deterministic=False)\n",
        "cfg.gpu_ids = range(1)\n",
        "\n",
        "cfg.lr_config = dict(\n",
        "    warmup=None,\n",
        "    warmup_iters=500,\n",
        "    warmup_ratio=0.001,\n",
        "    step=[8, 11],\n",
        "    policy='step')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XI1DKPcJE9rt",
        "outputId": "1f1bebb8-ce47-44ab-92a0-8369d6bfe0df"
      },
      "source": [
        "print(f'Config:\\n{cfg.pretty_text}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Config:\n",
            "model = dict(\n",
            "    type='CascadeRCNN',\n",
            "    pretrained='open-mmlab://msra/hrnetv2_w32',\n",
            "    backbone=dict(\n",
            "        type='HRNet',\n",
            "        extra=dict(\n",
            "            stage1=dict(\n",
            "                num_modules=1,\n",
            "                num_branches=1,\n",
            "                block='BOTTLENECK',\n",
            "                num_blocks=(4, ),\n",
            "                num_channels=(64, )),\n",
            "            stage2=dict(\n",
            "                num_modules=1,\n",
            "                num_branches=2,\n",
            "                block='BASIC',\n",
            "                num_blocks=(4, 4),\n",
            "                num_channels=(32, 64)),\n",
            "            stage3=dict(\n",
            "                num_modules=4,\n",
            "                num_branches=3,\n",
            "                block='BASIC',\n",
            "                num_blocks=(4, 4, 4),\n",
            "                num_channels=(32, 64, 128)),\n",
            "            stage4=dict(\n",
            "                num_modules=3,\n",
            "                num_branches=4,\n",
            "                block='BASIC',\n",
            "                num_blocks=(4, 4, 4, 4),\n",
            "                num_channels=(32, 64, 128, 256)))),\n",
            "    neck=dict(type='HRFPN', in_channels=[32, 64, 128, 256], out_channels=256),\n",
            "    rpn_head=dict(\n",
            "        type='RPNHead',\n",
            "        in_channels=256,\n",
            "        feat_channels=256,\n",
            "        anchor_generator=dict(\n",
            "            type='AnchorGenerator',\n",
            "            scales=[8],\n",
            "            ratios=[0.5, 1.0, 2.0],\n",
            "            strides=[4, 8, 16, 32, 64]),\n",
            "        bbox_coder=dict(\n",
            "            type='DeltaXYWHBBoxCoder',\n",
            "            target_means=[0.0, 0.0, 0.0, 0.0],\n",
            "            target_stds=[1.0, 1.0, 1.0, 1.0]),\n",
            "        loss_cls=dict(\n",
            "            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n",
            "        loss_bbox=dict(\n",
            "            type='SmoothL1Loss', beta=0.1111111111111111, loss_weight=1.0)),\n",
            "    roi_head=dict(\n",
            "        type='CascadeRoIHead',\n",
            "        num_stages=3,\n",
            "        stage_loss_weights=[1, 0.5, 0.25],\n",
            "        bbox_roi_extractor=dict(\n",
            "            type='SingleRoIExtractor',\n",
            "            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),\n",
            "            out_channels=256,\n",
            "            featmap_strides=[4, 8, 16, 32]),\n",
            "        bbox_head=[\n",
            "            dict(\n",
            "                type='Shared2FCBBoxHead',\n",
            "                in_channels=256,\n",
            "                fc_out_channels=1024,\n",
            "                roi_feat_size=7,\n",
            "                num_classes=2,\n",
            "                bbox_coder=dict(\n",
            "                    type='DeltaXYWHBBoxCoder',\n",
            "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
            "                    target_stds=[0.1, 0.1, 0.2, 0.2]),\n",
            "                reg_class_agnostic=True,\n",
            "                loss_cls=dict(\n",
            "                    type='CrossEntropyLoss',\n",
            "                    use_sigmoid=False,\n",
            "                    loss_weight=1.0),\n",
            "                loss_bbox=dict(type='SmoothL1Loss', beta=1.0,\n",
            "                               loss_weight=1.0)),\n",
            "            dict(\n",
            "                type='Shared2FCBBoxHead',\n",
            "                in_channels=256,\n",
            "                fc_out_channels=1024,\n",
            "                roi_feat_size=7,\n",
            "                num_classes=2,\n",
            "                bbox_coder=dict(\n",
            "                    type='DeltaXYWHBBoxCoder',\n",
            "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
            "                    target_stds=[0.05, 0.05, 0.1, 0.1]),\n",
            "                reg_class_agnostic=True,\n",
            "                loss_cls=dict(\n",
            "                    type='CrossEntropyLoss',\n",
            "                    use_sigmoid=False,\n",
            "                    loss_weight=1.0),\n",
            "                loss_bbox=dict(type='SmoothL1Loss', beta=1.0,\n",
            "                               loss_weight=1.0)),\n",
            "            dict(\n",
            "                type='Shared2FCBBoxHead',\n",
            "                in_channels=256,\n",
            "                fc_out_channels=1024,\n",
            "                roi_feat_size=7,\n",
            "                num_classes=2,\n",
            "                bbox_coder=dict(\n",
            "                    type='DeltaXYWHBBoxCoder',\n",
            "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
            "                    target_stds=[0.033, 0.033, 0.067, 0.067]),\n",
            "                reg_class_agnostic=True,\n",
            "                loss_cls=dict(\n",
            "                    type='CrossEntropyLoss',\n",
            "                    use_sigmoid=False,\n",
            "                    loss_weight=1.0),\n",
            "                loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0))\n",
            "        ]),\n",
            "    train_cfg=dict(\n",
            "        rpn=dict(\n",
            "            assigner=dict(\n",
            "                type='MaxIoUAssigner',\n",
            "                pos_iou_thr=0.7,\n",
            "                neg_iou_thr=0.3,\n",
            "                min_pos_iou=0.3,\n",
            "                match_low_quality=True,\n",
            "                ignore_iof_thr=-1),\n",
            "            sampler=dict(\n",
            "                type='RandomSampler',\n",
            "                num=256,\n",
            "                pos_fraction=0.5,\n",
            "                neg_pos_ub=-1,\n",
            "                add_gt_as_proposals=False),\n",
            "            allowed_border=0,\n",
            "            pos_weight=-1,\n",
            "            debug=False),\n",
            "        rpn_proposal=dict(\n",
            "            nms_pre=2000,\n",
            "            max_per_img=2000,\n",
            "            nms=dict(type='nms', iou_threshold=0.7),\n",
            "            min_bbox_size=0),\n",
            "        rcnn=[\n",
            "            dict(\n",
            "                assigner=dict(\n",
            "                    type='MaxIoUAssigner',\n",
            "                    pos_iou_thr=0.5,\n",
            "                    neg_iou_thr=0.5,\n",
            "                    min_pos_iou=0.5,\n",
            "                    match_low_quality=False,\n",
            "                    ignore_iof_thr=-1),\n",
            "                sampler=dict(\n",
            "                    type='RandomSampler',\n",
            "                    num=512,\n",
            "                    pos_fraction=0.25,\n",
            "                    neg_pos_ub=-1,\n",
            "                    add_gt_as_proposals=True),\n",
            "                pos_weight=-1,\n",
            "                debug=False),\n",
            "            dict(\n",
            "                assigner=dict(\n",
            "                    type='MaxIoUAssigner',\n",
            "                    pos_iou_thr=0.6,\n",
            "                    neg_iou_thr=0.6,\n",
            "                    min_pos_iou=0.6,\n",
            "                    match_low_quality=False,\n",
            "                    ignore_iof_thr=-1),\n",
            "                sampler=dict(\n",
            "                    type='RandomSampler',\n",
            "                    num=512,\n",
            "                    pos_fraction=0.25,\n",
            "                    neg_pos_ub=-1,\n",
            "                    add_gt_as_proposals=True),\n",
            "                pos_weight=-1,\n",
            "                debug=False),\n",
            "            dict(\n",
            "                assigner=dict(\n",
            "                    type='MaxIoUAssigner',\n",
            "                    pos_iou_thr=0.7,\n",
            "                    neg_iou_thr=0.7,\n",
            "                    min_pos_iou=0.7,\n",
            "                    match_low_quality=False,\n",
            "                    ignore_iof_thr=-1),\n",
            "                sampler=dict(\n",
            "                    type='RandomSampler',\n",
            "                    num=512,\n",
            "                    pos_fraction=0.25,\n",
            "                    neg_pos_ub=-1,\n",
            "                    add_gt_as_proposals=True),\n",
            "                pos_weight=-1,\n",
            "                debug=False)\n",
            "        ]),\n",
            "    test_cfg=dict(\n",
            "        rpn=dict(\n",
            "            nms_pre=1000,\n",
            "            max_per_img=1000,\n",
            "            nms=dict(type='nms', iou_threshold=0.7),\n",
            "            min_bbox_size=0),\n",
            "        rcnn=dict(\n",
            "            score_thr=0.05,\n",
            "            nms=dict(type='nms', iou_threshold=0.5),\n",
            "            max_per_img=100)))\n",
            "dataset_type = 'CocoDataset'\n",
            "data_root = 'data/coco/'\n",
            "img_norm_cfg = dict(\n",
            "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
            "train_pipeline = [\n",
            "    dict(type='LoadImageFromFile'),\n",
            "    dict(type='LoadAnnotations', with_bbox=True),\n",
            "    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n",
            "    dict(type='RandomFlip', flip_ratio=0.5),\n",
            "    dict(\n",
            "        type='Normalize',\n",
            "        mean=[123.675, 116.28, 103.53],\n",
            "        std=[58.395, 57.12, 57.375],\n",
            "        to_rgb=True),\n",
            "    dict(type='Pad', size_divisor=32),\n",
            "    dict(type='DefaultFormatBundle'),\n",
            "    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])\n",
            "]\n",
            "test_pipeline = [\n",
            "    dict(type='LoadImageFromFile'),\n",
            "    dict(\n",
            "        type='MultiScaleFlipAug',\n",
            "        img_scale=(1333, 800),\n",
            "        flip=False,\n",
            "        transforms=[\n",
            "            dict(type='Resize', keep_ratio=True),\n",
            "            dict(type='RandomFlip'),\n",
            "            dict(\n",
            "                type='Normalize',\n",
            "                mean=[123.675, 116.28, 103.53],\n",
            "                std=[58.395, 57.12, 57.375],\n",
            "                to_rgb=True),\n",
            "            dict(type='Pad', size_divisor=32),\n",
            "            dict(type='ImageToTensor', keys=['img']),\n",
            "            dict(type='Collect', keys=['img'])\n",
            "        ])\n",
            "]\n",
            "data = dict(\n",
            "    samples_per_gpu=2,\n",
            "    workers_per_gpu=2,\n",
            "    train=dict(\n",
            "        type='CocoDataset',\n",
            "        ann_file='coco/train.json',\n",
            "        img_prefix='VOC/jpegimages',\n",
            "        pipeline=[\n",
            "            dict(type='LoadImageFromFile'),\n",
            "            dict(type='LoadAnnotations', with_bbox=True),\n",
            "            dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n",
            "            dict(type='RandomFlip', flip_ratio=0.5),\n",
            "            dict(\n",
            "                type='Normalize',\n",
            "                mean=[123.675, 116.28, 103.53],\n",
            "                std=[58.395, 57.12, 57.375],\n",
            "                to_rgb=True),\n",
            "            dict(type='Pad', size_divisor=32),\n",
            "            dict(type='DefaultFormatBundle'),\n",
            "            dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels'])\n",
            "        ],\n",
            "        data_root='/content/drive/MyDrive/data',\n",
            "        classes=('station_names', 'table')),\n",
            "    val=dict(\n",
            "        type='CocoDataset',\n",
            "        ann_file='coco/val.json',\n",
            "        img_prefix='VOC/jpegimages',\n",
            "        pipeline=[\n",
            "            dict(type='LoadImageFromFile'),\n",
            "            dict(\n",
            "                type='MultiScaleFlipAug',\n",
            "                img_scale=(1333, 800),\n",
            "                flip=False,\n",
            "                transforms=[\n",
            "                    dict(type='Resize', keep_ratio=True),\n",
            "                    dict(type='RandomFlip'),\n",
            "                    dict(\n",
            "                        type='Normalize',\n",
            "                        mean=[123.675, 116.28, 103.53],\n",
            "                        std=[58.395, 57.12, 57.375],\n",
            "                        to_rgb=True),\n",
            "                    dict(type='Pad', size_divisor=32),\n",
            "                    dict(type='ImageToTensor', keys=['img']),\n",
            "                    dict(type='Collect', keys=['img'])\n",
            "                ])\n",
            "        ],\n",
            "        data_root='/content/drive/MyDrive/data',\n",
            "        classes=('station_names', 'table')),\n",
            "    test=dict(\n",
            "        type='CocoDataset',\n",
            "        ann_file='coco/test.json',\n",
            "        img_prefix='VOC/jpegimages',\n",
            "        pipeline=[\n",
            "            dict(type='LoadImageFromFile'),\n",
            "            dict(\n",
            "                type='MultiScaleFlipAug',\n",
            "                img_scale=(1333, 800),\n",
            "                flip=False,\n",
            "                transforms=[\n",
            "                    dict(type='Resize', keep_ratio=True),\n",
            "                    dict(type='RandomFlip'),\n",
            "                    dict(\n",
            "                        type='Normalize',\n",
            "                        mean=[123.675, 116.28, 103.53],\n",
            "                        std=[58.395, 57.12, 57.375],\n",
            "                        to_rgb=True),\n",
            "                    dict(type='Pad', size_divisor=32),\n",
            "                    dict(type='ImageToTensor', keys=['img']),\n",
            "                    dict(type='Collect', keys=['img'])\n",
            "                ])\n",
            "        ],\n",
            "        data_root='/content/drive/MyDrive/data',\n",
            "        classes=('station_names', 'table')))\n",
            "evaluation = dict(interval=3, metric='bbox', classwise=True)\n",
            "optimizer = dict(type='SGD', lr=0.0025, momentum=0.9, weight_decay=0.0001)\n",
            "optimizer_config = dict(grad_clip=None)\n",
            "lr_config = dict(\n",
            "    warmup=None,\n",
            "    warmup_iters=500,\n",
            "    warmup_ratio=0.001,\n",
            "    step=[8, 11],\n",
            "    policy='step')\n",
            "runner = dict(type='EpochBasedRunner', max_epochs=20)\n",
            "checkpoint_config = dict(interval=3)\n",
            "log_config = dict(interval=600, hooks=[dict(type='TextLoggerHook')])\n",
            "custom_hooks = [dict(type='NumClassCheckHook')]\n",
            "dist_params = dict(backend='nccl')\n",
            "log_level = 'INFO'\n",
            "load_from = '/content/drive/MyDrive/data/init_model/epoch_36.pth'\n",
            "resume_from = None\n",
            "workflow = [('train', 1)]\n",
            "classes = ('station_names', 'table')\n",
            "work_dir = '/content/drive/MyDrive/data/work_dir'\n",
            "seed = 0\n",
            "gpu_ids = range(0, 1)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ug3iNksGIzcL",
        "outputId": "997f296f-448a-4622-9b68-bb935528ed4d"
      },
      "source": [
        "import mmcv\n",
        "import os.path as osp\n",
        "# Build dataset\n",
        "datasets = [build_dataset(cfg.data.train)]\n",
        "\n",
        "# Build the detector\n",
        "model = build_detector( cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
        "# Add an attribute for visualization convenience\n",
        "model.CLASSES = datasets[0].CLASSES\n",
        "\n",
        "print(model.CLASSES)\n",
        "# Create work_dir\n",
        "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
        "train_detector(model, datasets, cfg, distributed=False, validate=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.27s)\n",
            "creating index...\n",
            "index created!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/mmdetection/mmdet/models/backbones/hrnet.py:283: UserWarning: DeprecationWarning: pretrained is a deprecated, please use \"init_cfg\" instead\n",
            "  warnings.warn('DeprecationWarning: pretrained is a deprecated, '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "('station_names', 'table')\n",
            "loading annotations into memory...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-30 21:52:32,309 - mmdet - INFO - load checkpoint from /content/drive/MyDrive/data/init_model/epoch_36.pth\n",
            "2021-05-30 21:52:32,310 - mmdet - INFO - Use load_from_local loader\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done (t=0.25s)\n",
            "creating index...\n",
            "index created!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-30 21:52:49,406 - mmdet - WARNING - The model and loaded state dict do not match exactly\n",
            "\n",
            "unexpected key in source state_dict: bbox_head.0.fc_cls.weight, bbox_head.0.fc_cls.bias, bbox_head.0.fc_reg.weight, bbox_head.0.fc_reg.bias, bbox_head.0.shared_fcs.0.weight, bbox_head.0.shared_fcs.0.bias, bbox_head.0.shared_fcs.1.weight, bbox_head.0.shared_fcs.1.bias, bbox_head.1.fc_cls.weight, bbox_head.1.fc_cls.bias, bbox_head.1.fc_reg.weight, bbox_head.1.fc_reg.bias, bbox_head.1.shared_fcs.0.weight, bbox_head.1.shared_fcs.0.bias, bbox_head.1.shared_fcs.1.weight, bbox_head.1.shared_fcs.1.bias, bbox_head.2.fc_cls.weight, bbox_head.2.fc_cls.bias, bbox_head.2.fc_reg.weight, bbox_head.2.fc_reg.bias, bbox_head.2.shared_fcs.0.weight, bbox_head.2.shared_fcs.0.bias, bbox_head.2.shared_fcs.1.weight, bbox_head.2.shared_fcs.1.bias, mask_head.0.convs.0.conv.weight, mask_head.0.convs.0.conv.bias, mask_head.0.convs.1.conv.weight, mask_head.0.convs.1.conv.bias, mask_head.0.convs.2.conv.weight, mask_head.0.convs.2.conv.bias, mask_head.0.convs.3.conv.weight, mask_head.0.convs.3.conv.bias, mask_head.0.upsample.weight, mask_head.0.upsample.bias, mask_head.0.conv_logits.weight, mask_head.0.conv_logits.bias, mask_head.1.convs.0.conv.weight, mask_head.1.convs.0.conv.bias, mask_head.1.convs.1.conv.weight, mask_head.1.convs.1.conv.bias, mask_head.1.convs.2.conv.weight, mask_head.1.convs.2.conv.bias, mask_head.1.convs.3.conv.weight, mask_head.1.convs.3.conv.bias, mask_head.1.upsample.weight, mask_head.1.upsample.bias, mask_head.1.conv_logits.weight, mask_head.1.conv_logits.bias, mask_head.2.convs.0.conv.weight, mask_head.2.convs.0.conv.bias, mask_head.2.convs.1.conv.weight, mask_head.2.convs.1.conv.bias, mask_head.2.convs.2.conv.weight, mask_head.2.convs.2.conv.bias, mask_head.2.convs.3.conv.weight, mask_head.2.convs.3.conv.bias, mask_head.2.upsample.weight, mask_head.2.upsample.bias, mask_head.2.conv_logits.weight, mask_head.2.conv_logits.bias\n",
            "\n",
            "missing keys in source state_dict: roi_head.bbox_head.0.fc_cls.weight, roi_head.bbox_head.0.fc_cls.bias, roi_head.bbox_head.0.fc_reg.weight, roi_head.bbox_head.0.fc_reg.bias, roi_head.bbox_head.0.shared_fcs.0.weight, roi_head.bbox_head.0.shared_fcs.0.bias, roi_head.bbox_head.0.shared_fcs.1.weight, roi_head.bbox_head.0.shared_fcs.1.bias, roi_head.bbox_head.1.fc_cls.weight, roi_head.bbox_head.1.fc_cls.bias, roi_head.bbox_head.1.fc_reg.weight, roi_head.bbox_head.1.fc_reg.bias, roi_head.bbox_head.1.shared_fcs.0.weight, roi_head.bbox_head.1.shared_fcs.0.bias, roi_head.bbox_head.1.shared_fcs.1.weight, roi_head.bbox_head.1.shared_fcs.1.bias, roi_head.bbox_head.2.fc_cls.weight, roi_head.bbox_head.2.fc_cls.bias, roi_head.bbox_head.2.fc_reg.weight, roi_head.bbox_head.2.fc_reg.bias, roi_head.bbox_head.2.shared_fcs.0.weight, roi_head.bbox_head.2.shared_fcs.0.bias, roi_head.bbox_head.2.shared_fcs.1.weight, roi_head.bbox_head.2.shared_fcs.1.bias\n",
            "\n",
            "2021-05-30 21:52:49,436 - mmdet - INFO - Start running, host: root@b1aeec1b9321, work_dir: /content/drive/MyDrive/data/work_dir\n",
            "2021-05-30 21:52:49,438 - mmdet - INFO - workflow: [('train', 1)], max: 20 epochs\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:2973: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 2.9 task/s, elapsed: 5s, ETA:     0s"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-30 21:55:57,596 - mmdet - INFO - Evaluating bbox...\n",
            "2021-05-30 21:55:57,642 - mmdet - INFO - \n",
            "+---------------+-------+----------+-------+\n",
            "| category      | AP    | category | AP    |\n",
            "+---------------+-------+----------+-------+\n",
            "| station_names | 0.472 | table    | 0.956 |\n",
            "+---------------+-------+----------+-------+\n",
            "2021-05-30 21:55:57,644 - mmdet - INFO - Saving checkpoint at 3 epochs\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.02s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.01s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.714\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.879\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.748\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.725\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.798\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.798\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.798\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.798\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-30 21:56:06,797 - mmdet - INFO - Epoch(val) [3][14]\tbbox_mAP: 0.7140, bbox_mAP_50: 0.8790, bbox_mAP_75: 0.7480, bbox_mAP_s: -1.0000, bbox_mAP_m: -1.0000, bbox_mAP_l: 0.7250, bbox_mAP_copypaste: 0.714 0.879 0.748 -1.000 -1.000 0.725\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 4.5 task/s, elapsed: 3s, ETA:     0s"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-30 21:59:14,694 - mmdet - INFO - Evaluating bbox...\n",
            "2021-05-30 21:59:14,733 - mmdet - INFO - \n",
            "+---------------+-------+----------+-------+\n",
            "| category      | AP    | category | AP    |\n",
            "+---------------+-------+----------+-------+\n",
            "| station_names | 0.682 | table    | 1.000 |\n",
            "+---------------+-------+----------+-------+\n",
            "2021-05-30 21:59:14,735 - mmdet - INFO - Saving checkpoint at 6 epochs\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.02s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.01s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.841\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.971\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.898\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.843\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.875\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.875\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.875\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-30 21:59:26,128 - mmdet - INFO - Epoch(val) [6][14]\tbbox_mAP: 0.8410, bbox_mAP_50: 0.9710, bbox_mAP_75: 0.8980, bbox_mAP_s: -1.0000, bbox_mAP_m: -1.0000, bbox_mAP_l: 0.8430, bbox_mAP_copypaste: 0.841 0.971 0.898 -1.000 -1.000 0.843\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 4.5 task/s, elapsed: 3s, ETA:     0s"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-30 22:02:33,099 - mmdet - INFO - Evaluating bbox...\n",
            "2021-05-30 22:02:33,139 - mmdet - INFO - \n",
            "+---------------+-------+----------+-------+\n",
            "| category      | AP    | category | AP    |\n",
            "+---------------+-------+----------+-------+\n",
            "| station_names | 0.803 | table    | 1.000 |\n",
            "+---------------+-------+----------+-------+\n",
            "2021-05-30 22:02:33,141 - mmdet - INFO - Saving checkpoint at 9 epochs\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.02s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.01s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.902\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 1.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.979\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.902\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.915\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.915\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.915\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.915\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-30 22:02:42,657 - mmdet - INFO - Epoch(val) [9][14]\tbbox_mAP: 0.9020, bbox_mAP_50: 1.0000, bbox_mAP_75: 0.9790, bbox_mAP_s: -1.0000, bbox_mAP_m: -1.0000, bbox_mAP_l: 0.9020, bbox_mAP_copypaste: 0.902 1.000 0.979 -1.000 -1.000 0.902\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 4.4 task/s, elapsed: 3s, ETA:     0s"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-30 22:05:49,858 - mmdet - INFO - Evaluating bbox...\n",
            "2021-05-30 22:05:49,896 - mmdet - INFO - \n",
            "+---------------+-------+----------+-------+\n",
            "| category      | AP    | category | AP    |\n",
            "+---------------+-------+----------+-------+\n",
            "| station_names | 0.817 | table    | 1.000 |\n",
            "+---------------+-------+----------+-------+\n",
            "2021-05-30 22:05:49,897 - mmdet - INFO - Saving checkpoint at 12 epochs\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.02s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.01s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.908\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.999\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.979\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.908\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.923\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.923\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.923\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.923\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-30 22:06:01,962 - mmdet - INFO - Epoch(val) [12][14]\tbbox_mAP: 0.9080, bbox_mAP_50: 0.9990, bbox_mAP_75: 0.9790, bbox_mAP_s: -1.0000, bbox_mAP_m: -1.0000, bbox_mAP_l: 0.9080, bbox_mAP_copypaste: 0.908 0.999 0.979 -1.000 -1.000 0.908\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 4.5 task/s, elapsed: 3s, ETA:     0s"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-30 22:09:09,790 - mmdet - INFO - Evaluating bbox...\n",
            "2021-05-30 22:09:09,830 - mmdet - INFO - \n",
            "+---------------+-------+----------+-------+\n",
            "| category      | AP    | category | AP    |\n",
            "+---------------+-------+----------+-------+\n",
            "| station_names | 0.812 | table    | 1.000 |\n",
            "+---------------+-------+----------+-------+\n",
            "2021-05-30 22:09:09,832 - mmdet - INFO - Saving checkpoint at 15 epochs\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.02s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.01s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.906\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 1.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.979\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.906\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.921\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.921\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.921\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.921\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-30 22:09:20,700 - mmdet - INFO - Epoch(val) [15][14]\tbbox_mAP: 0.9060, bbox_mAP_50: 1.0000, bbox_mAP_75: 0.9790, bbox_mAP_s: -1.0000, bbox_mAP_m: -1.0000, bbox_mAP_l: 0.9060, bbox_mAP_copypaste: 0.906 1.000 0.979 -1.000 -1.000 0.906\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 14/14, 4.5 task/s, elapsed: 3s, ETA:     0s"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-30 22:12:28,262 - mmdet - INFO - Evaluating bbox...\n",
            "2021-05-30 22:12:28,300 - mmdet - INFO - \n",
            "+---------------+-------+----------+-------+\n",
            "| category      | AP    | category | AP    |\n",
            "+---------------+-------+----------+-------+\n",
            "| station_names | 0.814 | table    | 1.000 |\n",
            "+---------------+-------+----------+-------+\n",
            "2021-05-30 22:12:28,302 - mmdet - INFO - Saving checkpoint at 18 epochs\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.02s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.01s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.907\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 1.000\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.979\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.907\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.921\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.921\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.921\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.921\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-30 22:12:36,739 - mmdet - INFO - Epoch(val) [18][14]\tbbox_mAP: 0.9070, bbox_mAP_50: 1.0000, bbox_mAP_75: 0.9790, bbox_mAP_s: -1.0000, bbox_mAP_m: -1.0000, bbox_mAP_l: 0.9070, bbox_mAP_copypaste: 0.907 1.000 0.979 -1.000 -1.000 0.907\n",
            "2021-05-30 22:14:39,597 - mmdet - INFO - Saving checkpoint at 20 epochs\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIvGOPHHJR-r",
        "outputId": "3f7a07bf-31a0-4e9e-9a34-d433fc42f110"
      },
      "source": [
        "img = mmcv.imread('/content/drive/MyDrive/data/VOC/jpegimages/120.png')\n",
        "\n",
        "checkpoint_file = '/content/drive/MyDrive/data/work_dir/latest.pth'\n",
        "\n",
        "#model.cfg = cfg\n",
        "# build the model from a config file and a checkpoint file\n",
        "model = init_detector(cfg, checkpoint_file, device='cuda:0')\n",
        "\n",
        "result = inference_detector(model, img)\n",
        "#show_result_pyplot(model, img, result)\n",
        "model.show_result(img,result, out_file = '/content/drive/MyDrive/data/result1.jpg')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Use load_from_local loader\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:2973: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tz-s_7IPa2v_"
      },
      "source": [
        "show_result_pyplot(model,img,result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6HZHb-wycZf"
      },
      "source": [
        "from mmdet.apis import init_detector, inference_detector, show_result_pyplot\n",
        "import mmcv\n",
        "# Load model\n",
        "config_file = '/content/drive/MyDrive/data/Training_config.py'\n",
        "checkpoint_file = '/content/drive/MyDrive/data/work_dir/latest.pth'\n",
        "model = init_detector(config_file, checkpoint_file, device='cuda:0')\n",
        "\n",
        "# Test a single image \n",
        "img = \"/content/drive/MyDrive/data/VOC/jpegimages/16.png\"\n",
        "\n",
        "# Run Inference\n",
        "result = inference_detector(model, img)\n",
        "\n",
        "# Visualization results\n",
        "model.show_result(img,result, out_file = '/content/drive/MyDrive/data/result1.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k21IDqDG0DjE"
      },
      "source": [
        "result[0][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oh1wEqlCm6nK"
      },
      "source": [
        "res_borderTable = []\n",
        "res_cell = []\n",
        "\n",
        "## for tables with borders\n",
        "for r in result[0]:\n",
        "    if r[4]>.85:\n",
        "        res_borderTable.append(r[:4].astype(int))\n",
        "    ## for cells\n",
        "for r in result[1]:\n",
        "    if r[4]>.85:\n",
        "        res_cell.append(r[:4].astype(int))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4W4f6Ef3DsG"
      },
      "source": [
        "print(res_borderTable)\n",
        "print(res_cell)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zF-72I-v3Nte"
      },
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import pytesseract\n",
        "\n",
        "\n",
        "img = cv2.imread(\"/content/drive/MyDrive/data/VOC/jpegimages/16.png\")\n",
        "#crop = img[171:277, 229:1455]\n",
        "\n",
        "sn = []\n",
        "table= []\n",
        "\n",
        "for i in res_borderTable:\n",
        "    crop = img[i[1]-10:i[3]+10, i[0]-10:i[2]+10]\n",
        "    cv2_imshow(crop)\n",
        "    text = pytesseract.image_to_string(crop, config=\"--psm 6 --oem 2 tessedit_char_blacklist=1234567890\", \n",
        "                                       lang='rus+eng')\n",
        "    sn.append(text)\n",
        "    print(text)\n",
        "\n",
        "config = \"-c tessedit_char_whitelist=0123456789.()- --oem 3 --psm 6\"\n",
        "for i in res_cell:\n",
        "    crop = img[i[1]-5:i[3]+5, i[0]-5:i[2]+5]\n",
        "    cv2_imshow(crop)\n",
        "    text = pytesseract.image_to_string(crop, config=config, lang='datarescue')\n",
        "    table.append(text)\n",
        "    print(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cz3kAKEbAFmZ"
      },
      "source": [
        "final_text = sn[0]+table[0]+sn[1]+table[1]\n",
        "print(final_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tuprVK98Mgu"
      },
      "source": [
        "print(sn[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7vKGkoGF55x"
      },
      "source": [
        "!pip install pytesseract\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install tesseract-ocr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmnuoH_5GfRT"
      },
      "source": [
        "!wget https://github.com/tesseract-ocr/tessdata/raw/master/rus.traineddata\n",
        "\n",
        "!sudo mv -v /content/datarescue.traineddata /usr/share/tesseract-ocr/4.00/tessdata/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIwDdWsM6K-T",
        "outputId": "da7d07ce-4180-4069-c252-177743c72cf0"
      },
      "source": [
        "!wget https://github.com/tesseract-ocr/tessdata/raw/master/eng.traineddata\n",
        "\n",
        "!sudo mv -v /content/eng.traineddata /usr/share/tesseract-ocr/4.00/tessdata/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-22 21:15:05--  https://github.com/tesseract-ocr/tessdata/raw/master/eng.traineddata\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/tesseract-ocr/tessdata/master/eng.traineddata [following]\n",
            "--2021-05-22 21:15:06--  https://raw.githubusercontent.com/tesseract-ocr/tessdata/master/eng.traineddata\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23466654 (22M) [application/octet-stream]\n",
            "Saving to: ‘eng.traineddata’\n",
            "\n",
            "eng.traineddata     100%[===================>]  22.38M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-05-22 21:15:06 (168 MB/s) - ‘eng.traineddata’ saved [23466654/23466654]\n",
            "\n",
            "renamed '/content/eng.traineddata' -> '/usr/share/tesseract-ocr/4.00/tessdata/eng.traineddata'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7B31HLC-7ntB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}